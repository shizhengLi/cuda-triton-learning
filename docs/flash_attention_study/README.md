# Flash Attention æ·±åº¦å­¦ä¹ æŒ‡å—

åŸºäº [tiny-flash-attention](https://github.com/66RING/tiny-flash-attention) é¡¹ç›®çš„æ·±åº¦å­¦ä¹ å’Œåˆ†ææ–‡æ¡£é›†åˆã€‚

**æ³¨æ„**ï¼šFlashMLA ç›¸å…³å†…å®¹å·²ç§»è‡³ä¸“é—¨çš„ [FlashMLA å­¦ä¹ æŒ‡å—](../flashmla_study/)ï¼ŒCuTLASS ç›¸å…³å†…å®¹è¯·å‚è€ƒ [CuTLASS å­¦ä¹ æŒ‡å—](../cutlass_study/)ã€‚

## ğŸ“š å­¦ä¹ æ–‡æ¡£æ¦‚è§ˆ

æœ¬å­¦ä¹ æŒ‡å—åŒ…å«å››ä¸ªæ ¸å¿ƒæ–‡æ¡£ï¼Œä¸“æ³¨äº Flash Attention ç®—æ³•æœ¬èº«çš„æ·±åº¦åˆ†æï¼š

### ğŸ”¬ [01. Flash Attention ç†è®ºåŸç†è¯¦è§£](./01_flash_attention_theory.md)

**æ ¸å¿ƒå†…å®¹**ï¼š
- Flash Attention çš„èƒŒæ™¯ä¸åŠ¨æœº
- ä¼ ç»Ÿ Attention çš„å†…å­˜ç“¶é¢ˆåˆ†æ
- Online Softmax ç®—æ³•æ¨å¯¼
- Flash Attention æ ¸å¿ƒç®—æ³•è¯¦è§£
- å†…å­˜è®¿é—®ä¼˜åŒ–åŸç†

**é€‚åˆè¯»è€…**ï¼š
- å¸Œæœ›æ·±å…¥ç†è§£ Flash Attention ç®—æ³•åŸç†çš„ç ”ç©¶è€…
- éœ€è¦æŒæ¡åœ¨çº¿ Softmax æ•°å­¦æ¨å¯¼çš„å¼€å‘è€…
- å¯¹ GPU å†…å­˜å±‚æ¬¡ç»“æ„æ„Ÿå…´è¶£çš„å·¥ç¨‹å¸ˆ

**å­¦ä¹ é‡ç‚¹**ï¼š
```
ç†è®ºåŸºç¡€ â†’ æ•°å­¦æ¨å¯¼ â†’ ç®—æ³•è®¾è®¡ â†’ æ­£ç¡®æ€§è¯æ˜
```

### âš–ï¸ [02. Flash Attention v1 vs v2 å¯¹æ¯”åˆ†æ](./02_flash_attention_v1_vs_v2.md)

**æ ¸å¿ƒå†…å®¹**ï¼š
- v1 å’Œ v2 çš„è®¾è®¡å·®å¼‚è¯¦è§£
- å¾ªç¯é¡ºåºäº¤æ¢çš„æ·±å±‚å½±å“
- å¹¶è¡Œåº¦å’Œæ€§èƒ½æå‡åˆ†æ
- å®ç°ç»†èŠ‚çš„ç³»ç»Ÿæ€§å¯¹æ¯”

**é€‚åˆè¯»è€…**ï¼š
- éœ€è¦é€‰æ‹©åˆé€‚ Flash Attention ç‰ˆæœ¬çš„å¼€å‘è€…
- å¯¹ç®—æ³•å·¥ç¨‹ä¼˜åŒ–æ„Ÿå…´è¶£çš„ç ”ç©¶è€…
- è¿½æ±‚æ€§èƒ½æè‡´ä¼˜åŒ–çš„ç³»ç»Ÿå·¥ç¨‹å¸ˆ

**å­¦ä¹ é‡ç‚¹**ï¼š
```
ç®—æ³•æ¼”è¿› â†’ æ€§èƒ½åˆ†æ â†’ å·¥ç¨‹æƒè¡¡ â†’ æœ€ä½³å®è·µ
```

### ğŸ”§ [03. Tiny é¡¹ç›®å®ç°æ·±åº¦åˆ†æ](./03_tiny_implementation_analysis.md)

**æ ¸å¿ƒå†…å®¹**ï¼š
- Pythonã€Tritonã€CUDA ä¸‰ç§å®ç°çš„è¯¦ç»†è§£æ
- æ€§èƒ½å¯¹æ¯”å’Œç“¶é¢ˆåˆ†æ
- ä¸åŒå®ç°æ–¹å¼çš„ä¼˜ç¼ºç‚¹è¯„ä¼°
- æ¸è¿›å¼å­¦ä¹ è·¯å¾„è®¾è®¡

**é€‚åˆè¯»è€…**ï¼š
- å¸Œæœ›é€šè¿‡ä»£ç å­¦ä¹ ç®—æ³•çš„å¼€å‘è€…
- éœ€è¦ç†è§£ä¸åŒæ¡†æ¶ç‰¹ç‚¹çš„å·¥ç¨‹å¸ˆ
- è®¡åˆ’å®ç°è‡ªå·±çš„ Flash Attention çš„ç ”ç©¶è€…

**å­¦ä¹ é‡ç‚¹**ï¼š
```
ä»£ç åˆ†æ â†’ æ€§èƒ½æµ‹è¯• â†’ å®ç°å¯¹æ¯” â†’ å®è·µæŒ‡å¯¼
```

### ğŸš€ [04. ä¼˜åŒ–æŒ‡å—ä¸æ”¹è¿›æ–¹å‘](./04_optimization_guide.md)

**æ ¸å¿ƒå†…å®¹**ï¼š
- å¤šå±‚æ¬¡ä¼˜åŒ–ç­–ç•¥è¯¦è§£
- åŸºäº tiny é¡¹ç›®çš„å…·ä½“ä¼˜åŒ–å»ºè®®
- ç¡¬ä»¶é€‚é…å’Œç³»ç»Ÿçº§ä¼˜åŒ–
- å‰æ²¿ç ”ç©¶æ–¹å‘å’Œå®é™…æ¡ˆä¾‹

**é€‚åˆè¯»è€…**ï¼š
- éœ€è¦ä¼˜åŒ–ç°æœ‰å®ç°çš„å·¥ç¨‹å¸ˆ
- å¯¹å‰æ²¿ä¼˜åŒ–æŠ€æœ¯æ„Ÿå…´è¶£çš„ç ”ç©¶è€…
- è´Ÿè´£ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²çš„ç³»ç»Ÿæ¶æ„å¸ˆ

**å­¦ä¹ é‡ç‚¹**ï¼š
```
æ€§èƒ½åˆ†æ â†’ ä¼˜åŒ–ç­–ç•¥ â†’ å·¥ç¨‹å®è·µ â†’ å‰æ²¿æ¢ç´¢
```

## ğŸ¯ å»ºè®®å­¦ä¹ è·¯å¾„

### åˆå­¦è€…è·¯å¾„ï¼ˆç®—æ³•ç†è§£ä¸ºä¸»ï¼‰
```
01 ç†è®ºåŸç† â†’ 03 å®ç°åˆ†æ (Pythonéƒ¨åˆ†) â†’ 02 ç‰ˆæœ¬å¯¹æ¯” â†’ 04 ä¼˜åŒ–æŒ‡å— (å‰åŠéƒ¨åˆ†)
```

### è¿›é˜¶è·¯å¾„ï¼ˆå®ç°èƒ½åŠ›ä¸ºä¸»ï¼‰
```
01 ç†è®ºåŸç† â†’ 03 å®ç°åˆ†æ (å…¨éƒ¨) â†’ 04 ä¼˜åŒ–æŒ‡å— â†’ 02 ç‰ˆæœ¬å¯¹æ¯”
```

### ç ”ç©¶è·¯å¾„ï¼ˆå‰æ²¿æ¢ç´¢ä¸ºä¸»ï¼‰
```
01 ç†è®ºåŸç† â†’ 02 ç‰ˆæœ¬å¯¹æ¯” â†’ 04 ä¼˜åŒ–æŒ‡å— â†’ 03 å®ç°åˆ†æ
```

### å·¥ç¨‹è·¯å¾„ï¼ˆç”Ÿäº§åº”ç”¨ä¸ºä¸»ï¼‰
```
03 å®ç°åˆ†æ â†’ 04 ä¼˜åŒ–æŒ‡å— â†’ FlashMLA å­¦ä¹ æŒ‡å— â†’ CuTLASS å­¦ä¹ æŒ‡å—
```

## ğŸ“ é…å¥—èµ„æº

### é¡¹ç›®ç»“æ„å¯¹åº”å…³ç³»
```
å­¦ä¹ æ–‡æ¡£                    â†â†’    å¯¹åº”é¡¹ç›®
â”œâ”€â”€ 01_theory.md           â†â†’    ç†è®ºåŸºç¡€ (é€šç”¨)
â”œâ”€â”€ 02_v1_vs_v2.md         â†â†’    Flash Attention è®ºæ–‡
â”œâ”€â”€ 03_implementation.md   â†â†’    tiny-flash-attention/
â””â”€â”€ 04_optimization.md     â†â†’    ä¼˜åŒ–å®è·µ (é€šç”¨)
```

### å®é™…é¡¹ç›®ä½ç½®
```
../../../
â”œâ”€â”€ tiny-flash-attention/     # æ•™è‚²æ€§å¤šè¯­è¨€å®ç°
â”‚   â”œâ”€â”€ flash_attention_py/   # Python å’Œ Triton å®ç°
â”‚   â”œâ”€â”€ flash_attention_cuda/ # CUDA å®ç°
â”‚   â””â”€â”€ flash_attention_cutlass/ # CuTLASS å®ç°
â”œâ”€â”€ FlashMLA/                  # DeepSeek ç”Ÿäº§çº§å®ç°
â””â”€â”€ cutlass_basics/            # CuTLASS å­¦ä¹ ç¤ºä¾‹
```

## ğŸ”— ç›¸å…³å­¦ä¹ æŒ‡å—

### [FlashMLA å­¦ä¹ æŒ‡å—](../flashmla_study/)
**ä¸“æ³¨äº**ï¼šDeepSeek çš„ç”Ÿäº§çº§ MLA å®ç°
- MLA æ¶æ„æ·±åº¦è§£æ
- ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–æŠ€æœ¯
- Hopper GPU ç‰¹åŒ–ä¼˜åŒ–
- å˜é•¿åºåˆ—æœåŠ¡å®è·µ

### [CuTLASS å­¦ä¹ æŒ‡å—](../cutlass_study/)
**ä¸“æ³¨äº**ï¼šCuTLASS 3.x é«˜æ€§èƒ½ç¼–ç¨‹
- CuTe å¼ é‡ç¼–ç¨‹èŒƒå¼
- TMA å†…å­˜åŠ é€Ÿå™¨
- Swizzling ä¼˜åŒ–æŠ€æœ¯
- FlashMLA å®ç°åŸºç¡€

## ğŸ› ï¸ å®è·µå»ºè®®

### ç¯å¢ƒé…ç½®
```bash
# æ¿€æ´»æ­£ç¡®çš„ conda ç¯å¢ƒ
conda activate agent

# æµ‹è¯• tiny-flash-attention
cd ../../../tiny-flash-attention/flash_attention_py
make

# æµ‹è¯•é¡¹ç›®ä¸­çš„å®ç°
cd ../../../flash_attention/naive
python naive_attention.py
```

### å¯¹æ¯”å­¦ä¹ å®éªŒ
```python
def comprehensive_comparison():
    """å…¨é¢çš„ Flash Attention å®ç°å¯¹æ¯”"""
    
    # æµ‹è¯•é…ç½®
    configs = [
        (1024, 64, 8),    # å°è§„æ¨¡
        (2048, 128, 12),  # ä¸­ç­‰è§„æ¨¡  
        (4096, 128, 16),  # å¤§è§„æ¨¡
    ]
    
    implementations = {
        'naive': benchmark_naive_attention,
        'tiny_python': benchmark_tiny_python,
        'tiny_triton': benchmark_tiny_triton,
        'project_triton': benchmark_project_triton,
    }
    
    results = {}
    for seq_len, head_dim, num_heads in configs:
        config_results = {}
        for name, benchmark_func in implementations.items():
            try:
                time_ms, memory_mb = benchmark_func(seq_len, head_dim, num_heads)
                config_results[name] = {
                    'time': time_ms,
                    'memory': memory_mb,
                    'status': 'success'
                }
            except Exception as e:
                config_results[name] = {
                    'status': 'failed',
                    'error': str(e)
                }
        
        results[f"{seq_len}x{head_dim}x{num_heads}"] = config_results
    
    return results
```

### å­¦ä¹ å®éªŒå»ºè®®
1. **ç†è®ºéªŒè¯**ï¼šå®ç° Online Softmax å¹¶éªŒè¯æ•°å€¼æ­£ç¡®æ€§
2. **æ€§èƒ½åˆ†æ**ï¼šä½¿ç”¨ profiling å·¥å…·åˆ†æä¸åŒå®ç°çš„ç“¶é¢ˆ
3. **å‚æ•°è°ƒä¼˜**ï¼šå°è¯•ä¸åŒçš„å—å¤§å°å’Œé…ç½®å‚æ•°
4. **ç®—æ³•æ”¹è¿›**ï¼šåŸºäºç†è®ºç†è§£å®ç°ä¼˜åŒ–ç‰ˆæœ¬

## ğŸ“Š å­¦ä¹ æˆæœæ£€éªŒ

### åŸºç¡€ç†è®ºæŒæ¡ (Level 1)
- [ ] ç†è§£ Flash Attention è§£å†³çš„æ ¸å¿ƒé—®é¢˜
- [ ] æŒæ¡ Online Softmax çš„æ•°å­¦æ¨å¯¼
- [ ] äº†è§£åˆ†å—è®¡ç®—çš„å†…å­˜ä¼˜åŒ–åŸç†
- [ ] ç†è§£ä¸åŒç‰ˆæœ¬çš„æ¼”è¿›é€»è¾‘

### å®ç°èƒ½åŠ›æŒæ¡ (Level 2)
- [ ] èƒ½å¤Ÿè¿è¡Œå’Œä¿®æ”¹ tiny-flash-attention ä»£ç 
- [ ] ç†è§£ Python/Triton/CUDA ä¸‰ç§å®ç°çš„å·®å¼‚
- [ ] èƒ½å¤Ÿè¿›è¡ŒåŸºç¡€çš„æ€§èƒ½æµ‹è¯•å’Œåˆ†æ
- [ ] æŒæ¡åŸºæœ¬çš„ GPU ç¼–ç¨‹æ¦‚å¿µ

### ä¼˜åŒ–å®è·µèƒ½åŠ› (Level 3)
- [ ] èƒ½å¤Ÿåˆ†æå’Œè¯†åˆ«æ€§èƒ½ç“¶é¢ˆ
- [ ] ç†è§£ç¡¬ä»¶æ¶æ„å¯¹ç®—æ³•å®ç°çš„å½±å“
- [ ] æŒæ¡ç³»ç»Ÿçº§ä¼˜åŒ–çš„åŸºæœ¬æ€è·¯
- [ ] èƒ½å¤Ÿé€‰æ‹©é€‚åˆç‰¹å®šåœºæ™¯çš„å®ç°æ–¹æ¡ˆ

### é«˜çº§åº”ç”¨èƒ½åŠ› (Level 4)
- [ ] èƒ½å¤Ÿè®¾è®¡æ–°çš„ Attention å˜ä½“
- [ ] æŒæ¡å¤šç§å®ç°æ¡†æ¶çš„ä¼˜ç¼ºç‚¹
- [ ] å…·å¤‡è§£å†³å¤æ‚å·¥ç¨‹é—®é¢˜çš„èƒ½åŠ›
- [ ] ç†è§£å‰æ²¿ç ”ç©¶æ–¹å‘å’Œå‘å±•è¶‹åŠ¿

## å­¦ä¹ èµ„æº

### æ ¸å¿ƒè®ºæ–‡
- [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
- [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/abs/2307.08691)
- [Online normalizer calculation for softmax](https://arxiv.org/abs/1805.02867)

### æŠ€æœ¯æ–‡æ¡£
- [CUDA ç¼–ç¨‹æŒ‡å—](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)
- [Triton æ–‡æ¡£](https://triton-lang.org/)
- [Flash Attention å®˜æ–¹å®ç°](https://github.com/Dao-AILab/flash-attention)

### æ€§èƒ½å¯¹æ¯”

| å®ç°æ–¹å¼ | ç›®æ ‡åœºæ™¯ | æ€§èƒ½ç‰¹ç‚¹ | å­¦ä¹ ä»·å€¼ |
|----------|----------|----------|----------|
| **tiny-flash-attention** | æ•™è‚²å­¦ä¹  | æ¸…æ™°æ˜“æ‡‚ | â­â­â­â­â­ |
| **é¡¹ç›®å®ç°** | ç®—æ³•éªŒè¯ | æ¸è¿›ä¼˜åŒ– | â­â­â­â­ |
| **Flash Attention å®˜æ–¹** | ç ”ç©¶å¼€å‘ | åŠŸèƒ½å®Œæ•´ | â­â­â­ |

## è‡´è°¢

### æ ¸å¿ƒè´¡çŒ®è€…
- **[@66RING](https://github.com/66RING)** - [tiny-flash-attention](https://github.com/66RING/tiny-flash-attention) é¡¹ç›®ä½œè€…ï¼Œä¸ºæœ¬å­¦ä¹ é¡¹ç›®æä¾›äº†å®è´µçš„æ•™è‚²èµ„æº

### å­¦æœ¯è‡´è°¢
- **Tri Dao** ç­‰äººçš„ Flash Attention åŸåˆ›æ€§å·¥ä½œ
- **OpenAI Triton** å›¢é˜Ÿæ¨åŠ¨çš„ GPU ç¼–ç¨‹é©æ–°
- **NVIDIA** åœ¨ CUDA ç”Ÿæ€ç³»ç»Ÿæ–¹é¢çš„æŒç»­æŠ•å…¥

## ä½¿ç”¨æŒ‡å—

### å­¦ä¹ å»ºè®®
1. **åˆå­¦è€…**ï¼šä» `01_flash_attention_theory.md` çš„ç†è®ºæ–‡æ¡£å¼€å§‹
2. **è¿›é˜¶è€…**ï¼šåˆ†æ `tiny-flash-attention` çš„å¤šç§å®ç°
3. **å·¥ç¨‹å¸ˆ**ï¼šç»“åˆ FlashMLA å’Œ CuTLASS å­¦ä¹ æŒ‡å—è¿›è¡Œæ·±åº¦å­¦ä¹ 

### å®éªŒå»ºè®®
```bash
# å¯¹æ¯”ä¸åŒå®ç°çš„æ€§èƒ½
cd ../../benchmarks
python benchmark_flash_attention.py

# åˆ†ææ€§èƒ½ç“¶é¢ˆ
nsys profile python flash_attention_benchmark.py
```

### å¼€å‘å®è·µ
```bash
# å®ç°è‡ªå·±çš„ä¼˜åŒ–ç‰ˆæœ¬
cp tiny-flash-attention/flash_attention_py/tiny_flash_attn.py my_implementation.py
# åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œä¼˜åŒ–å’Œæ”¹è¿›
```

---

ğŸš€ **Flash Attention ç®—æ³•æ·±åº¦å­¦ä¹ ï¼**

æœ¬æŒ‡å—ä¸“æ³¨äº Flash Attention ç®—æ³•æœ¬èº«çš„ç†è®ºå’Œå®ç°ï¼Œä¸ºæ‚¨æä¾›ä»æ•°å­¦åŸç†åˆ°ä»£ç å®ç°çš„å®Œæ•´å­¦ä¹ è·¯å¾„ã€‚

*æœ€åæ›´æ–°ï¼š2024å¹´12æœˆ - è°ƒæ•´é¡¹ç›®ç»“æ„ï¼Œä¸“æ³¨äº Flash Attention ç®—æ³•*

