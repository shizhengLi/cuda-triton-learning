# Flash Attention æ·±åº¦å­¦ä¹ æŒ‡å—

åŸºäº [tiny-flash-attention](https://github.com/66RING/tiny-flash-attention) å’Œ [FlashMLA](https://github.com/deepseek-ai/FlashMLA) é¡¹ç›®çš„æ·±åº¦å­¦ä¹ å’Œåˆ†ææ–‡æ¡£é›†åˆã€‚

## ğŸ“š å­¦ä¹ æ–‡æ¡£æ¦‚è§ˆ

æœ¬å­¦ä¹ æŒ‡å—åŒ…å«äº”ä¸ªç›¸äº’å…³è”çš„æ–‡æ¡£ï¼Œä»ç†è®ºåŸºç¡€åˆ°ç”Ÿäº§å®è·µï¼Œæä¾›äº†å®Œæ•´çš„ Flash Attention å­¦ä¹ è·¯å¾„ï¼š

### ğŸ”¬ [01. Flash Attention ç†è®ºåŸç†è¯¦è§£](./01_flash_attention_theory.md)

**æ ¸å¿ƒå†…å®¹**ï¼š
- Flash Attention çš„èƒŒæ™¯ä¸åŠ¨æœº
- ä¼ ç»Ÿ Attention çš„å†…å­˜ç“¶é¢ˆåˆ†æ
- Online Softmax ç®—æ³•æ¨å¯¼
- Flash Attention æ ¸å¿ƒç®—æ³•è¯¦è§£
- å†…å­˜è®¿é—®ä¼˜åŒ–åŸç†

**é€‚åˆè¯»è€…**ï¼š
- å¸Œæœ›æ·±å…¥ç†è§£ Flash Attention ç®—æ³•åŸç†çš„ç ”ç©¶è€…
- éœ€è¦æŒæ¡åœ¨çº¿ Softmax æ•°å­¦æ¨å¯¼çš„å¼€å‘è€…
- å¯¹ GPU å†…å­˜å±‚æ¬¡ç»“æ„æ„Ÿå…´è¶£çš„å·¥ç¨‹å¸ˆ

**å­¦ä¹ é‡ç‚¹**ï¼š
```
ç†è®ºåŸºç¡€ â†’ æ•°å­¦æ¨å¯¼ â†’ ç®—æ³•è®¾è®¡ â†’ æ­£ç¡®æ€§è¯æ˜
```

### âš–ï¸ [02. Flash Attention v1 vs v2 å¯¹æ¯”åˆ†æ](./02_flash_attention_v1_vs_v2.md)

**æ ¸å¿ƒå†…å®¹**ï¼š
- v1 å’Œ v2 çš„è®¾è®¡å·®å¼‚è¯¦è§£
- å¾ªç¯é¡ºåºäº¤æ¢çš„æ·±å±‚å½±å“
- å¹¶è¡Œåº¦å’Œæ€§èƒ½æå‡åˆ†æ
- å®ç°ç»†èŠ‚çš„ç³»ç»Ÿæ€§å¯¹æ¯”

**é€‚åˆè¯»è€…**ï¼š
- éœ€è¦é€‰æ‹©åˆé€‚ Flash Attention ç‰ˆæœ¬çš„å¼€å‘è€…
- å¯¹ç®—æ³•å·¥ç¨‹ä¼˜åŒ–æ„Ÿå…´è¶£çš„ç ”ç©¶è€…
- è¿½æ±‚æ€§èƒ½æè‡´ä¼˜åŒ–çš„ç³»ç»Ÿå·¥ç¨‹å¸ˆ

**å­¦ä¹ é‡ç‚¹**ï¼š
```
ç®—æ³•æ¼”è¿› â†’ æ€§èƒ½åˆ†æ â†’ å·¥ç¨‹æƒè¡¡ â†’ æœ€ä½³å®è·µ
```

### ğŸ”§ [03. Tiny é¡¹ç›®å®ç°æ·±åº¦åˆ†æ](./03_tiny_implementation_analysis.md)

**æ ¸å¿ƒå†…å®¹**ï¼š
- Pythonã€Tritonã€CUDA ä¸‰ç§å®ç°çš„è¯¦ç»†è§£æ
- æ€§èƒ½å¯¹æ¯”å’Œç“¶é¢ˆåˆ†æ
- ä¸åŒå®ç°æ–¹å¼çš„ä¼˜ç¼ºç‚¹è¯„ä¼°
- æ¸è¿›å¼å­¦ä¹ è·¯å¾„è®¾è®¡

**é€‚åˆè¯»è€…**ï¼š
- å¸Œæœ›é€šè¿‡ä»£ç å­¦ä¹ ç®—æ³•çš„å¼€å‘è€…
- éœ€è¦ç†è§£ä¸åŒæ¡†æ¶ç‰¹ç‚¹çš„å·¥ç¨‹å¸ˆ
- è®¡åˆ’å®ç°è‡ªå·±çš„ Flash Attention çš„ç ”ç©¶è€…

**å­¦ä¹ é‡ç‚¹**ï¼š
```
ä»£ç åˆ†æ â†’ æ€§èƒ½æµ‹è¯• â†’ å®ç°å¯¹æ¯” â†’ å®è·µæŒ‡å¯¼
```

### ğŸš€ [04. ä¼˜åŒ–æŒ‡å—ä¸æ”¹è¿›æ–¹å‘](./04_optimization_guide.md)

**æ ¸å¿ƒå†…å®¹**ï¼š
- å¤šå±‚æ¬¡ä¼˜åŒ–ç­–ç•¥è¯¦è§£
- åŸºäº tiny é¡¹ç›®çš„å…·ä½“ä¼˜åŒ–å»ºè®®
- ç¡¬ä»¶é€‚é…å’Œç³»ç»Ÿçº§ä¼˜åŒ–
- å‰æ²¿ç ”ç©¶æ–¹å‘å’Œå®é™…æ¡ˆä¾‹

**é€‚åˆè¯»è€…**ï¼š
- éœ€è¦ä¼˜åŒ–ç°æœ‰å®ç°çš„å·¥ç¨‹å¸ˆ
- å¯¹å‰æ²¿ä¼˜åŒ–æŠ€æœ¯æ„Ÿå…´è¶£çš„ç ”ç©¶è€…
- è´Ÿè´£ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²çš„ç³»ç»Ÿæ¶æ„å¸ˆ

**å­¦ä¹ é‡ç‚¹**ï¼š
```
æ€§èƒ½åˆ†æ â†’ ä¼˜åŒ–ç­–ç•¥ â†’ å·¥ç¨‹å®è·µ â†’ å‰æ²¿æ¢ç´¢
```

### ğŸ­ [05. FlashMLA ç”Ÿäº§çº§å®ç°åˆ†æ](./05_flashmla_production_analysis.md) **NEW!**

**æ ¸å¿ƒå†…å®¹**ï¼š
- DeepSeek FlashMLA çš„ MLA æ¶æ„è§£æ
- ç”Ÿäº§çº§é«˜æ€§èƒ½å®ç°çš„æ ¸å¿ƒæŠ€æœ¯
- Hopper GPU æ¶æ„çš„æ·±åº¦ä¼˜åŒ–
- å˜é•¿åºåˆ—æœåŠ¡çš„å·¥ç¨‹å®è·µ

**é€‚åˆè¯»è€…**ï¼š
- å¸Œæœ›äº†è§£æœ€æ–°ç”Ÿäº§çº§å®ç°çš„å¼€å‘è€…
- éœ€è¦éƒ¨ç½²é«˜æ€§èƒ½æ¨ç†ç³»ç»Ÿçš„å·¥ç¨‹å¸ˆ
- å¯¹æœ€æ–° GPU æ¶æ„ä¼˜åŒ–æ„Ÿå…´è¶£çš„ç ”ç©¶è€…

**å­¦ä¹ é‡ç‚¹**ï¼š
```
MLA æ¶æ„ â†’ ç”Ÿäº§ä¼˜åŒ– â†’ ç¡¬ä»¶é€‚é… â†’ å·¥ç¨‹å®è·µ
```

## ğŸ¯ å»ºè®®å­¦ä¹ è·¯å¾„

### åˆå­¦è€…è·¯å¾„ï¼ˆç®—æ³•ç†è§£ä¸ºä¸»ï¼‰
```
01 ç†è®ºåŸç† â†’ 03 å®ç°åˆ†æ (Pythonéƒ¨åˆ†) â†’ 02 ç‰ˆæœ¬å¯¹æ¯” â†’ 04 ä¼˜åŒ–æŒ‡å— (å‰åŠéƒ¨åˆ†)
```

### è¿›é˜¶è·¯å¾„ï¼ˆå®ç°èƒ½åŠ›ä¸ºä¸»ï¼‰
```
01 ç†è®ºåŸç† â†’ 03 å®ç°åˆ†æ (å…¨éƒ¨) â†’ 04 ä¼˜åŒ–æŒ‡å— â†’ 05 FlashMLA åˆ†æ â†’ 02 ç‰ˆæœ¬å¯¹æ¯”
```

### ä¸“å®¶è·¯å¾„ï¼ˆç”Ÿäº§ä¼˜åŒ–ä¸ºä¸»ï¼‰
```
05 FlashMLA åˆ†æ â†’ 04 ä¼˜åŒ–æŒ‡å— â†’ 03 å®ç°åˆ†æ (CUDAéƒ¨åˆ†) â†’ 02 ç‰ˆæœ¬å¯¹æ¯” â†’ 01 ç†è®ºåŸç†
```

### ç ”ç©¶è·¯å¾„ï¼ˆå‰æ²¿æ¢ç´¢ä¸ºä¸»ï¼‰
```
01 ç†è®ºåŸç† â†’ 02 ç‰ˆæœ¬å¯¹æ¯” â†’ 05 FlashMLA åˆ†æ â†’ 04 ä¼˜åŒ–æŒ‡å— â†’ 03 å®ç°åˆ†æ
```

## ğŸ“ é…å¥—èµ„æº

### é¡¹ç›®ç»“æ„å¯¹åº”å…³ç³»
```
å­¦ä¹ æ–‡æ¡£                    â†â†’    å¯¹åº”é¡¹ç›®
â”œâ”€â”€ 01_theory.md           â†â†’    ç†è®ºåŸºç¡€ (é€šç”¨)
â”œâ”€â”€ 02_v1_vs_v2.md         â†â†’    Flash Attention è®ºæ–‡
â”œâ”€â”€ 03_implementation.md   â†â†’    tiny-flash-attention/
â”œâ”€â”€ 04_optimization.md     â†â†’    ä¼˜åŒ–å®è·µ (é€šç”¨)
â””â”€â”€ 05_flashmla.md         â†â†’    FlashMLA/
```

### å®é™…é¡¹ç›®ä½ç½®
```
../../../
â”œâ”€â”€ tiny-flash-attention/     # æ•™è‚²æ€§å¤šè¯­è¨€å®ç°
â”‚   â”œâ”€â”€ flash_attention_py/   # Python å’Œ Triton å®ç°
â”‚   â”œâ”€â”€ flash_attention_cuda/ # CUDA å®ç°
â”‚   â””â”€â”€ flash_attention_cutlass/ # CuTLASS å®ç°
â””â”€â”€ FlashMLA/                  # DeepSeek ç”Ÿäº§çº§å®ç°
    â”œâ”€â”€ csrc/                  # CUDA C++ æ ¸å¿ƒ
    â”œâ”€â”€ flash_mla/             # Python æ¥å£
    â””â”€â”€ benchmark/             # æ€§èƒ½æµ‹è¯•
```

## ğŸ› ï¸ å®è·µå»ºè®®

### ç¯å¢ƒé…ç½®
```bash
# æ¿€æ´»æ­£ç¡®çš„ conda ç¯å¢ƒ
conda activate agent

# æµ‹è¯• tiny-flash-attention
cd ../../../tiny-flash-attention/flash_attention_py
make

# æµ‹è¯• FlashMLA (éœ€è¦ Hopper GPU)
cd ../../../FlashMLA
python tests/test_flash_mla.py
```

### å¯¹æ¯”å­¦ä¹ å®éªŒ
```python
# æ€§èƒ½å¯¹æ¯”å®éªŒè®¾è®¡
import torch
import time

def benchmark_tiny_python(seq_len, head_dim, num_heads):
    """Benchmark tiny-flash-attention Python implementation"""
    from tiny_flash_attn import flash_attn_v1
    q = torch.randn(1, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float16)
    k = torch.randn(1, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float16)
    v = torch.randn(1, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float16)
    return benchmark(flash_attn_v1, q, k, v)

def benchmark_tiny_triton(seq_len, head_dim, num_heads):
    """Benchmark tiny-flash-attention Triton implementation"""
    from tiny_flash_attn_triton import flash_attn_triton
    q = torch.randn(1, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float16)
    k = torch.randn(1, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float16)
    v = torch.randn(1, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float16)
    return benchmark(flash_attn_triton, q, k, v)

def benchmark_flash_mla(seq_len, head_dim, num_heads):
    """Benchmark FlashMLA implementation (requires Hopper GPU)"""
    from flash_mla import flash_mla_v1
    q = torch.randn(1, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float16)
    k = torch.randn(1, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float16)
    v = torch.randn(1, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float16)
    return benchmark(flash_mla_v1, q, k, v)

def has_hopper_gpu():
    """Check if Hopper GPU is available"""
    try:
        import torch
        torch.cuda.get_device_name(0) == "Hopper"
        return True
    except:
        return False

def benchmark(func, *args, num_runs=10):
    # é¢„çƒ­
    for _ in range(3):
        _ = func(*args)
    torch.cuda.synchronize()
    
    # æµ‹è¯•
    start_time = time.time()
    for _ in range(num_runs):
        result = func(*args)
    torch.cuda.synchronize()
    
    avg_time = (time.time() - start_time) / num_runs * 1000  # ms
    return avg_time, result

def compare_implementations():
    """å¯¹æ¯”ä¸åŒå®ç°çš„æ€§èƒ½ç‰¹ç‚¹"""
    
    # æµ‹è¯•é…ç½®
    configs = [
        (1024, 64, 8),    # å°è§„æ¨¡
        (2048, 128, 12),  # ä¸­ç­‰è§„æ¨¡  
        (4096, 128, 16),  # å¤§è§„æ¨¡
    ]
    
    results = {}
    for seq_len, head_dim, num_heads in configs:
        print(f"Testing: seq_len={seq_len}, head_dim={head_dim}, num_heads={num_heads}")
        
        # 1. tiny-flash-attention Python å®ç°
        tiny_time, _ = benchmark_tiny_python(seq_len, head_dim, num_heads)
        
        # 2. tiny-flash-attention Triton å®ç°  
        triton_time, _ = benchmark_tiny_triton(seq_len, head_dim, num_heads)
        
        # 3. FlashMLA å®ç° (å¦‚æœæœ‰ Hopper GPU)
        if has_hopper_gpu():
            mla_time, _ = benchmark_flash_mla(seq_len, head_dim, num_heads)
        else:
            mla_time = None
            
        results[f"{seq_len}x{head_dim}x{num_heads}"] = {
            'tiny_python': tiny_time,
            'tiny_triton': triton_time, 
            'flash_mla': mla_time,
            'triton_speedup': tiny_time / triton_time if triton_time else None,
            'mla_speedup': tiny_time / mla_time if mla_time else None
        }
    
    return results

# è¿è¡Œå¯¹æ¯”æµ‹è¯•
performance_comparison = compare_implementations()
```

### å­¦ä¹ å®éªŒå»ºè®®
1. **ç†è®ºéªŒè¯**ï¼šå®ç° Online Softmax å¹¶éªŒè¯æ•°å€¼æ­£ç¡®æ€§
2. **æ€§èƒ½åˆ†æ**ï¼šä½¿ç”¨ profiling å·¥å…·åˆ†æä¸åŒå®ç°çš„ç“¶é¢ˆ
3. **å‚æ•°è°ƒä¼˜**ï¼šå°è¯•ä¸åŒçš„å—å¤§å°å’Œé…ç½®å‚æ•°
4. **ç¡¬ä»¶å¯¹æ¯”**ï¼šåœ¨ä¸åŒ GPU ä¸Šæµ‹è¯•æ€§èƒ½å·®å¼‚

## ğŸ“Š å­¦ä¹ æˆæœæ£€éªŒ

### åŸºç¡€ç†è®ºæŒæ¡ (Level 1)
- [ ] ç†è§£ Flash Attention è§£å†³çš„æ ¸å¿ƒé—®é¢˜
- [ ] æŒæ¡ Online Softmax çš„æ•°å­¦æ¨å¯¼
- [ ] äº†è§£åˆ†å—è®¡ç®—çš„å†…å­˜ä¼˜åŒ–åŸç†
- [ ] ç†è§£ä¸åŒç‰ˆæœ¬çš„æ¼”è¿›é€»è¾‘

### å®ç°èƒ½åŠ›æŒæ¡ (Level 2)
- [ ] èƒ½å¤Ÿè¿è¡Œå’Œä¿®æ”¹ tiny-flash-attention ä»£ç 
- [ ] ç†è§£ Python/Triton/CUDA ä¸‰ç§å®ç°çš„å·®å¼‚
- [ ] èƒ½å¤Ÿè¿›è¡ŒåŸºç¡€çš„æ€§èƒ½æµ‹è¯•å’Œåˆ†æ
- [ ] æŒæ¡åŸºæœ¬çš„ GPU ç¼–ç¨‹æ¦‚å¿µ

### ä¼˜åŒ–å®è·µèƒ½åŠ› (Level 3)
- [ ] èƒ½å¤Ÿåˆ†æå’Œè¯†åˆ«æ€§èƒ½ç“¶é¢ˆ
- [ ] ç†è§£ç¡¬ä»¶æ¶æ„å¯¹ç®—æ³•å®ç°çš„å½±å“
- [ ] æŒæ¡ç³»ç»Ÿçº§ä¼˜åŒ–çš„åŸºæœ¬æ€è·¯
- [ ] èƒ½å¤Ÿé€‰æ‹©é€‚åˆç‰¹å®šåœºæ™¯çš„å®ç°æ–¹æ¡ˆ

### ç”Ÿäº§åº”ç”¨èƒ½åŠ› (Level 4)
- [ ] ç†è§£ FlashMLA çš„ç”Ÿäº§çº§ç‰¹æ€§
- [ ] æŒæ¡å˜é•¿åºåˆ—ä¼˜åŒ–çš„æ ¸å¿ƒæŠ€æœ¯
- [ ] äº†è§£ä¼ä¸šçº§éƒ¨ç½²çš„è€ƒè™‘å› ç´ 
- [ ] èƒ½å¤Ÿè®¾è®¡å®Œæ•´çš„æ€§èƒ½ç›‘æ§æ–¹æ¡ˆ

## ğŸ”„ é¡¹ç›®æ›´æ–°

### æœ€æ–°æ›´æ–° (2024.12)
- âœ… æ·»åŠ  FlashMLA ç”Ÿäº§çº§å®ç°åˆ†æ
- âœ… æ•´åˆ DeepSeek çš„æœ€æ–°ä¼˜åŒ–æŠ€æœ¯
- âœ… æ›´æ–°å­¦ä¹ è·¯å¾„ï¼ŒåŒ…å«ç”Ÿäº§å®è·µ
- âœ… å¢åŠ å¤šç¡¬ä»¶å¹³å°æ”¯æŒä¿¡æ¯

### è®¡åˆ’æ›´æ–°
- ğŸ”„ MLA æ¶æ„çš„è¯¦ç»†æ•°å­¦æ¨å¯¼
- ğŸ”„ Hopper GPU ç‰¹æ€§çš„æ·±åº¦è§£æ
- ğŸ”„ å˜é•¿åºåˆ—ä¼˜åŒ–çš„å®è·µæ¡ˆä¾‹
- ğŸ”„ å¤šå‚å•† GPU çš„æ€§èƒ½å¯¹æ¯”

## ğŸ”— æ‰©å±•èµ„æº

### å­¦æœ¯è®ºæ–‡
- [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
- [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/abs/2307.08691)
- [Multi-Head Latent Attention for Neural Machine Translation](https://arxiv.org/abs/2410.04343)
- [Online normalizer calculation for softmax](https://arxiv.org/abs/1805.02867)

### å¼€æºé¡¹ç›®
- [Flash Attention å®˜æ–¹å®ç°](https://github.com/Dao-AILab/flash-attention)
- [tiny-flash-attention](https://github.com/66RING/tiny-flash-attention)
- [FlashMLA](https://github.com/deepseek-ai/FlashMLA)
- [Triton å®˜æ–¹æ–‡æ¡£](https://triton-lang.org/)

### æŠ€æœ¯åšå®¢å’Œæ•™ç¨‹
- [FlashMLA æŠ€æœ¯æ·±åº¦è§£æ](https://github.com/deepseek-ai/FlashMLA/blob/main/docs/20250422-new-kernel-deep-dive.md)
- [NVIDIA Hopper æ¶æ„ç™½çš®ä¹¦](https://www.nvidia.com/en-us/data-center/h100/)
- [CUDA ç¼–ç¨‹æŒ‡å—](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)

## ğŸ’¡ è´¡çŒ®å’Œåé¦ˆ

æœ¬å­¦ä¹ æŒ‡å—åŸºäºå¤šä¸ªå¼€æºé¡¹ç›®çš„æ·±åº¦åˆ†æå’Œæ€»ç»“ã€‚æ¬¢è¿ï¼š

1. æå‡ºå…·ä½“çš„ä¿®æ”¹å»ºè®®å’Œå†…å®¹è¡¥å……
2. åˆ†äº«æ‚¨çš„å­¦ä¹ ä½“éªŒå’Œå®è·µå¿ƒå¾—
3. è´¡çŒ®æ–°çš„å®ç°ç¤ºä¾‹å’Œæµ‹è¯•ç»“æœ
4. æ¨èç›¸å…³çš„å­¦ä¹ èµ„æºå’Œæœ€æ–°ç ”ç©¶

## ğŸ† è‡´è°¢

### é¡¹ç›®è´¡çŒ®è€…
- **[@66RING](https://github.com/66RING)** - [tiny-flash-attention](https://github.com/66RING/tiny-flash-attention) æ•™è‚²èµ„æº
- **[DeepSeek AI](https://github.com/deepseek-ai)** - [FlashMLA](https://github.com/deepseek-ai/FlashMLA) ç”Ÿäº§çº§å®ç°

### å­¦æœ¯è‡´è°¢
- **Tri Dao** ç­‰äººçš„ Flash Attention åŸåˆ›æ€§å·¥ä½œ
- **DeepSeek** å›¢é˜Ÿåœ¨ MLA æ¶æ„æ–¹é¢çš„åˆ›æ–°è´¡çŒ®
- **OpenAI Triton** å›¢é˜Ÿæ¨åŠ¨çš„ GPU ç¼–ç¨‹é©æ–°
- **NVIDIA** åœ¨ CUDA ç”Ÿæ€ç³»ç»Ÿå’Œç¡¬ä»¶åˆ›æ–°æ–¹é¢çš„æŠ•å…¥

---

**ğŸš€ å¼€å§‹æ‚¨çš„ Flash Attention æ·±åº¦å­¦ä¹ ä¹‹æ—…ï¼**

ä»ç†è®ºåŸºç¡€åˆ°ç”Ÿäº§å®è·µï¼Œè¿™å¥—å­¦ä¹ æŒ‡å—å°†å¸®åŠ©æ‚¨ç³»ç»ŸæŒæ¡ç°ä»£é«˜æ€§èƒ½ GPU ç¼–ç¨‹çš„æ ¸å¿ƒæŠ€æœ¯ã€‚å»ºè®®ä» [ç†è®ºåŸç†è¯¦è§£](./01_flash_attention_theory.md) å¼€å§‹ï¼Œå»ºç«‹æ‰å®çš„ç†è®ºåŸºç¡€ã€‚

*æœ€åæ›´æ–°ï¼š2024å¹´12æœˆ - æ–°å¢ FlashMLA ç”Ÿäº§çº§å®ç°åˆ†æ*

