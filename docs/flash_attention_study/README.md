# Flash Attention æ·±åº¦å­¦ä¹ æŒ‡å—

åŸºäº [tiny-flash-attention](https://github.com/66RING/tiny-flash-attention) é¡¹ç›®çš„æ·±åº¦å­¦ä¹ å’Œåˆ†ææ–‡æ¡£é›†åˆã€‚

## ğŸ“š å­¦ä¹ æ–‡æ¡£æ¦‚è§ˆ

æœ¬å­¦ä¹ æŒ‡å—åŒ…å«å››ä¸ªç›¸äº’å…³è”çš„æ–‡æ¡£ï¼Œä»ç†è®ºåŸºç¡€åˆ°å®é™…ä¼˜åŒ–ï¼Œæä¾›äº†å®Œæ•´çš„ Flash Attention å­¦ä¹ è·¯å¾„ï¼š

### ğŸ”¬ [01. Flash Attention ç†è®ºåŸç†è¯¦è§£](./01_flash_attention_theory.md)

**æ ¸å¿ƒå†…å®¹**ï¼š
- Flash Attention çš„èƒŒæ™¯ä¸åŠ¨æœº
- ä¼ ç»Ÿ Attention çš„å†…å­˜ç“¶é¢ˆåˆ†æ
- Online Softmax ç®—æ³•æ¨å¯¼
- Flash Attention æ ¸å¿ƒç®—æ³•è¯¦è§£
- å†…å­˜è®¿é—®ä¼˜åŒ–åŸç†

**é€‚åˆè¯»è€…**ï¼š
- å¸Œæœ›æ·±å…¥ç†è§£ Flash Attention ç®—æ³•åŸç†çš„ç ”ç©¶è€…
- éœ€è¦æŒæ¡åœ¨çº¿ Softmax æ•°å­¦æ¨å¯¼çš„å¼€å‘è€…
- å¯¹ GPU å†…å­˜å±‚æ¬¡ç»“æ„æ„Ÿå…´è¶£çš„å·¥ç¨‹å¸ˆ

**å­¦ä¹ é‡ç‚¹**ï¼š
```
ç†è®ºåŸºç¡€ â†’ æ•°å­¦æ¨å¯¼ â†’ ç®—æ³•è®¾è®¡ â†’ æ­£ç¡®æ€§è¯æ˜
```

### âš–ï¸ [02. Flash Attention v1 vs v2 å¯¹æ¯”åˆ†æ](./02_flash_attention_v1_vs_v2.md)

**æ ¸å¿ƒå†…å®¹**ï¼š
- v1 å’Œ v2 çš„è®¾è®¡å·®å¼‚è¯¦è§£
- å¾ªç¯é¡ºåºäº¤æ¢çš„æ·±å±‚å½±å“
- å¹¶è¡Œåº¦å’Œæ€§èƒ½æå‡åˆ†æ
- å®ç°ç»†èŠ‚çš„ç³»ç»Ÿæ€§å¯¹æ¯”

**é€‚åˆè¯»è€…**ï¼š
- éœ€è¦é€‰æ‹©åˆé€‚ Flash Attention ç‰ˆæœ¬çš„å¼€å‘è€…
- å¯¹ç®—æ³•å·¥ç¨‹ä¼˜åŒ–æ„Ÿå…´è¶£çš„ç ”ç©¶è€…
- è¿½æ±‚æ€§èƒ½æè‡´ä¼˜åŒ–çš„ç³»ç»Ÿå·¥ç¨‹å¸ˆ

**å­¦ä¹ é‡ç‚¹**ï¼š
```
ç®—æ³•æ¼”è¿› â†’ æ€§èƒ½åˆ†æ â†’ å·¥ç¨‹æƒè¡¡ â†’ æœ€ä½³å®è·µ
```

### ğŸ”§ [03. Tiny é¡¹ç›®å®ç°æ·±åº¦åˆ†æ](./03_tiny_implementation_analysis.md)

**æ ¸å¿ƒå†…å®¹**ï¼š
- Pythonã€Tritonã€CUDA ä¸‰ç§å®ç°çš„è¯¦ç»†è§£æ
- æ€§èƒ½å¯¹æ¯”å’Œç“¶é¢ˆåˆ†æ
- ä¸åŒå®ç°æ–¹å¼çš„ä¼˜ç¼ºç‚¹è¯„ä¼°
- æ¸è¿›å¼å­¦ä¹ è·¯å¾„è®¾è®¡

**é€‚åˆè¯»è€…**ï¼š
- å¸Œæœ›é€šè¿‡ä»£ç å­¦ä¹ ç®—æ³•çš„å¼€å‘è€…
- éœ€è¦ç†è§£ä¸åŒæ¡†æ¶ç‰¹ç‚¹çš„å·¥ç¨‹å¸ˆ
- è®¡åˆ’å®ç°è‡ªå·±çš„ Flash Attention çš„ç ”ç©¶è€…

**å­¦ä¹ é‡ç‚¹**ï¼š
```
ä»£ç åˆ†æ â†’ æ€§èƒ½æµ‹è¯• â†’ å®ç°å¯¹æ¯” â†’ å®è·µæŒ‡å¯¼
```

### ğŸš€ [04. ä¼˜åŒ–æŒ‡å—ä¸æ”¹è¿›æ–¹å‘](./04_optimization_guide.md)

**æ ¸å¿ƒå†…å®¹**ï¼š
- å¤šå±‚æ¬¡ä¼˜åŒ–ç­–ç•¥è¯¦è§£
- åŸºäº tiny é¡¹ç›®çš„å…·ä½“ä¼˜åŒ–å»ºè®®
- ç¡¬ä»¶é€‚é…å’Œç³»ç»Ÿçº§ä¼˜åŒ–
- å‰æ²¿ç ”ç©¶æ–¹å‘å’Œå®é™…æ¡ˆä¾‹

**é€‚åˆè¯»è€…**ï¼š
- éœ€è¦ä¼˜åŒ–ç°æœ‰å®ç°çš„å·¥ç¨‹å¸ˆ
- å¯¹å‰æ²¿ä¼˜åŒ–æŠ€æœ¯æ„Ÿå…´è¶£çš„ç ”ç©¶è€…
- è´Ÿè´£ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²çš„ç³»ç»Ÿæ¶æ„å¸ˆ

**å­¦ä¹ é‡ç‚¹**ï¼š
```
æ€§èƒ½åˆ†æ â†’ ä¼˜åŒ–ç­–ç•¥ â†’ å·¥ç¨‹å®è·µ â†’ å‰æ²¿æ¢ç´¢
```

## ğŸ¯ å»ºè®®å­¦ä¹ è·¯å¾„

### åˆå­¦è€…è·¯å¾„ï¼ˆç®—æ³•ç†è§£ä¸ºä¸»ï¼‰
```
01 ç†è®ºåŸç† â†’ 03 å®ç°åˆ†æ (Pythonéƒ¨åˆ†) â†’ 02 ç‰ˆæœ¬å¯¹æ¯” â†’ 04 ä¼˜åŒ–æŒ‡å— (å‰åŠéƒ¨åˆ†)
```

### è¿›é˜¶è·¯å¾„ï¼ˆå®ç°èƒ½åŠ›ä¸ºä¸»ï¼‰
```
01 ç†è®ºåŸç† â†’ 03 å®ç°åˆ†æ (å…¨éƒ¨) â†’ 04 ä¼˜åŒ–æŒ‡å— â†’ 02 ç‰ˆæœ¬å¯¹æ¯”
```

### ä¸“å®¶è·¯å¾„ï¼ˆä¼˜åŒ–æ€§èƒ½ä¸ºä¸»ï¼‰
```
02 ç‰ˆæœ¬å¯¹æ¯” â†’ 04 ä¼˜åŒ–æŒ‡å— â†’ 03 å®ç°åˆ†æ (CUDAéƒ¨åˆ†) â†’ 01 ç†è®ºåŸç† (ä½œä¸ºå‚è€ƒ)
```

## ğŸ“ é…å¥—èµ„æº

### tiny-flash-attention é¡¹ç›®ç»“æ„
```
../../../tiny-flash-attention/
â”œâ”€â”€ flash_attention_py/          # Python å’Œ Triton å®ç°
â”œâ”€â”€ flash_attention_cuda/        # CUDA å®ç°
â”œâ”€â”€ flash_attention_cutlass/     # CuTLASS å®ç°
â””â”€â”€ flash_attention_c/           # C è¯­è¨€å®ç°
```

### ç›¸å…³ä»£ç ç¤ºä¾‹
æ‰€æœ‰æ–‡æ¡£ä¸­çš„ä»£ç ç¤ºä¾‹éƒ½åŸºäº tiny-flash-attention é¡¹ç›®ï¼Œå¯ä»¥ç›´æ¥è¿è¡Œå’Œå®éªŒã€‚

## ğŸ› ï¸ å®è·µå»ºè®®

### ç¯å¢ƒé…ç½®
```bash
# æ¿€æ´»æ­£ç¡®çš„ conda ç¯å¢ƒ
conda activate agent

# è¿›å…¥ tiny-flash-attention ç›®å½•
cd /data/lishizheng/cpp_projects/cuda-triton-learning/tiny-flash-attention

# è¿è¡Œ Python å®ç°æµ‹è¯•
cd flash_attention_py
python tiny_flash_attn.py

# è¿è¡Œ Triton å®ç°æµ‹è¯•
make  # æˆ–è€… python -m pytest -s tiny_flash_attn_triton.py
```

### æ€§èƒ½æµ‹è¯•
```python
# åŸºç¡€æ€§èƒ½å¯¹æ¯”
from tiny_flash_attn import flash_attn_v1
from tiny_flash_attn_triton import flash_attn_triton
import torch
import time

# åˆ›å»ºæµ‹è¯•æ•°æ®
batch_size, num_heads, seq_len, head_dim = 8, 12, 1024, 64
q = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float16)
k = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float16)
v = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float16)

# æ€§èƒ½æµ‹è¯•
def benchmark(func, *args, num_runs=10):
    # é¢„çƒ­
    for _ in range(3):
        _ = func(*args)
    torch.cuda.synchronize()
    
    # æµ‹è¯•
    start_time = time.time()
    for _ in range(num_runs):
        result = func(*args)
    torch.cuda.synchronize()
    
    avg_time = (time.time() - start_time) / num_runs * 1000  # ms
    return avg_time, result

# å¯¹æ¯”æµ‹è¯•
python_time, python_result = benchmark(flash_attn_v1, q, k, v)
triton_time, triton_result = benchmark(flash_attn_triton, q, k, v)

print(f"Python å®ç°: {python_time:.2f} ms")
print(f"Triton å®ç°: {triton_time:.2f} ms")
print(f"åŠ é€Ÿæ¯”: {python_time/triton_time:.2f}x")
```

### å­¦ä¹ å®éªŒ
1. **ç®—æ³•éªŒè¯**ï¼šä¿®æ”¹ tiny é¡¹ç›®ä¸­çš„å‚æ•°ï¼Œè§‚å¯Ÿå¯¹æ€§èƒ½å’Œç²¾åº¦çš„å½±å“
2. **æ€§èƒ½åˆ†æ**ï¼šä½¿ç”¨ `torch.profiler` æˆ– `nsys` åˆ†æç“¶é¢ˆ
3. **ä¼˜åŒ–å®è·µ**ï¼šå®ç°æ–‡æ¡£ä¸­æåˆ°çš„ä¼˜åŒ–å»ºè®®
4. **åˆ›æ–°æ¢ç´¢**ï¼šåŸºäºç†è§£å®ç°è‡ªå·±çš„ä¼˜åŒ–ç‰ˆæœ¬

## ğŸ“Š å­¦ä¹ æˆæœæ£€éªŒ

### ç†è®ºæŒæ¡åº¦æµ‹è¯•
- [ ] èƒ½å¤Ÿè§£é‡Š Flash Attention è§£å†³çš„æ ¸å¿ƒé—®é¢˜
- [ ] ç†è§£ Online Softmax çš„æ•°å­¦æ¨å¯¼
- [ ] æŒæ¡åˆ†å—è®¡ç®—çš„å†…å­˜ä¼˜åŒ–åŸç†
- [ ] äº†è§£ v1 å’Œ v2 çš„å…³é”®å·®å¼‚

### å®è·µèƒ½åŠ›æµ‹è¯•  
- [ ] èƒ½å¤Ÿè¿è¡Œå’Œä¿®æ”¹ tiny é¡¹ç›®çš„ä»£ç 
- [ ] å®ç°åŸºç¡€çš„æ€§èƒ½æµ‹è¯•å’Œå¯¹æ¯”
- [ ] èƒ½å¤Ÿè¯†åˆ«å’Œåˆ†ææ€§èƒ½ç“¶é¢ˆ
- [ ] å°è¯•å®ç°ç®€å•çš„ä¼˜åŒ–æ”¹è¿›

### åº”ç”¨æ°´å¹³æµ‹è¯•
- [ ] èƒ½å¤Ÿä¸ºç‰¹å®šåœºæ™¯é€‰æ‹©åˆé€‚çš„å®ç°æ–¹å¼
- [ ] ç†è§£åœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„éƒ¨ç½²è€ƒè™‘
- [ ] æŒæ¡ç³»ç»Ÿçº§ä¼˜åŒ–çš„åŸºæœ¬æ€è·¯
- [ ] äº†è§£å‰æ²¿ç ”ç©¶æ–¹å‘å’Œå‘å±•è¶‹åŠ¿

## ğŸ”— æ‰©å±•èµ„æº

### å­¦æœ¯è®ºæ–‡
- [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
- [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/abs/2307.08691)
- [Online normalizer calculation for softmax](https://arxiv.org/abs/1805.02867)

### å¼€æºé¡¹ç›®
- [Flash Attention å®˜æ–¹å®ç°](https://github.com/Dao-AILab/flash-attention)
- [Triton å®˜æ–¹æ–‡æ¡£](https://triton-lang.org/)
- [CUDA ç¼–ç¨‹æŒ‡å—](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)

### ç›¸å…³è¯¾ç¨‹å’Œæ•™ç¨‹
- [GPU å¹¶è¡Œç¼–ç¨‹ä¸“é¡¹è¯¾ç¨‹](../learning_plan.md)
- [CUDA åŸºç¡€ç¤ºä¾‹](../../../cuda_basics/)
- [Triton å…¥é—¨æ•™ç¨‹](../../../triton_basics/)

## ğŸ’¡ è´¡çŒ®å’Œåé¦ˆ

æœ¬å­¦ä¹ æŒ‡å—æ˜¯åŸºäº tiny-flash-attention é¡¹ç›®çš„æ·±åº¦åˆ†æå’Œæ€»ç»“ã€‚å¦‚æœæ‚¨å‘ç°å†…å®¹é”™è¯¯ã€æœ‰æ”¹è¿›å»ºè®®æˆ–å¸Œæœ›è¡¥å……æ–°çš„å†…å®¹ï¼Œæ¬¢è¿ï¼š

1. æå‡ºå…·ä½“çš„ä¿®æ”¹å»ºè®®
2. åˆ†äº«æ‚¨çš„å­¦ä¹ ä½“éªŒå’Œå¿ƒå¾—
3. è´¡çŒ®æ–°çš„ä¼˜åŒ–å®ç°å’Œæµ‹è¯•ç»“æœ
4. æ¨èç›¸å…³çš„å­¦ä¹ èµ„æº

## ğŸ† è‡´è°¢

ç‰¹åˆ«æ„Ÿè°¢ï¼š
- [tiny-flash-attention](https://github.com/66RING/tiny-flash-attention) é¡¹ç›®æä¾›çš„ä¼˜ç§€å®ç°å’Œå­¦ä¹ èµ„æº
- Flash Attention åŸä½œè€… Tri Dao ç­‰äººçš„å¼€åˆ›æ€§å·¥ä½œ
- CUDA å¹¶è¡Œç¼–ç¨‹å’Œ GPU ä¼˜åŒ–ç¤¾åŒºçš„çŸ¥è¯†åˆ†äº«

---

**å¼€å§‹æ‚¨çš„ Flash Attention å­¦ä¹ ä¹‹æ—…å§ï¼** ğŸš€

å»ºè®®ä» [ç†è®ºåŸç†è¯¦è§£](./01_flash_attention_theory.md) å¼€å§‹ï¼Œå»ºç«‹æ‰å®çš„ç†è®ºåŸºç¡€ï¼Œç„¶åç»“åˆå®é™…ä»£ç åŠ æ·±ç†è§£ã€‚

