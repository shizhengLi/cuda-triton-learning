# CUDA & Triton Learning: Flash Attention ä¸é«˜æ€§èƒ½ç®—å­å®ç°

æœ¬é¡¹ç›®è‡´åŠ›äºæ·±å…¥å­¦ä¹ é«˜æ€§èƒ½ GPU ç®—å­çš„è®¾è®¡ä¸å®ç°ï¼Œä»¥ Flash Attention ä¸ºæ ¸å¿ƒæ¡ˆä¾‹ï¼Œé€šè¿‡åˆ†æå¤šç§å®ç°æ–¹å¼æ¥æŒæ¡ CUDA å¹¶è¡Œç¼–ç¨‹å’Œ Triton ç®—å­å¼€å‘æŠ€èƒ½ã€‚

## é¡¹ç›®ç‰¹è‰²

- **ç†è®ºä¸å®è·µç»“åˆ**ï¼šä»æ•°å­¦åŸç†åˆ°å·¥ç¨‹å®ç°çš„å®Œæ•´å­¦ä¹ è·¯å¾„
- **å¤šæ¡†æ¶å¯¹æ¯”**ï¼šPythonã€Tritonã€CUDA ä¸‰ç§å®ç°æ–¹å¼çš„æ·±åº¦åˆ†æ
- **å‰æ²¿æŠ€æœ¯æ¢ç´¢**ï¼šé›†æˆ DeepSeek FlashMLA ç­‰ä¸šç•Œæœ€æ–°é«˜æ€§èƒ½å®ç°
- **ç³»ç»Ÿæ€§ä¼˜åŒ–**ï¼šä»ç®—æ³•åˆ°ç¡¬ä»¶çš„å¤šå±‚æ¬¡æ€§èƒ½ä¼˜åŒ–

## æ ¸å¿ƒå†…å®¹

### Flash Attention æ·±åº¦å­¦ä¹ 
åŸºäº tiny-flash-attention é¡¹ç›®çš„ç³»ç»Ÿæ€§å­¦ä¹ ææ–™ï¼š
- **ç†è®ºåŸç†**ï¼šOnline Softmaxã€åˆ†å—è®¡ç®—ã€å†…å­˜ä¼˜åŒ–
- **ç‰ˆæœ¬æ¼”è¿›**ï¼šFlash Attention v1 vs v2 çš„è¯¦ç»†å¯¹æ¯”
- **å®ç°åˆ†æ**ï¼šPython/Triton/CUDA ä¸‰ç§å®ç°çš„æ·±åº¦è§£æ
- **ä¼˜åŒ–æŒ‡å—**ï¼šæ€§èƒ½è°ƒä¼˜å’Œå·¥ç¨‹å®è·µ

### é«˜æ€§èƒ½ Kernel å®ç°
åŸºäº DeepSeek FlashMLA çš„å‰æ²¿æŠ€æœ¯å­¦ä¹ ï¼š
- **MLA æ¶æ„**ï¼šMulti-Head Latent Attention çš„è®¾è®¡åŸç†
- **Hopper ä¼˜åŒ–**ï¼šé’ˆå¯¹æœ€æ–° GPU æ¶æ„çš„ä¸“é—¨ä¼˜åŒ–
- **æè‡´æ€§èƒ½**ï¼šåœ¨ H800 SXM5 ä¸Šè¾¾åˆ° 660 TFLOPS è®¡ç®—æ€§èƒ½
- **å·¥ç¨‹å®è·µ**ï¼šå˜é•¿åºåˆ—æœåŠ¡çš„ç”Ÿäº§çº§å®ç°

## é¡¹ç›®ç»“æ„

```
cuda-triton-learning/
â”œâ”€â”€ docs/                         # è¯¦ç»†å­¦ä¹ æ–‡æ¡£
â”‚   â”œâ”€â”€ flash_attention_study/    # Flash Attention æ·±åº¦å­¦ä¹ æŒ‡å—
â”‚   â”‚   â”œâ”€â”€ 01_theory.md         # ç†è®ºåŸç†è¯¦è§£
â”‚   â”‚   â”œâ”€â”€ 02_v1_vs_v2.md       # ç‰ˆæœ¬å¯¹æ¯”åˆ†æ
â”‚   â”‚   â”œâ”€â”€ 03_implementation.md  # å®ç°æ·±åº¦åˆ†æ
â”‚   â”‚   â””â”€â”€ 04_optimization.md   # ä¼˜åŒ–æŒ‡å—
â”‚   â””â”€â”€ learning_plan.md          # 11å‘¨å­¦ä¹ è®¡åˆ’
â”‚
â”œâ”€â”€ cuda_basics/                  # CUDA ç¼–ç¨‹åŸºç¡€
â”‚   â”œâ”€â”€ 01_hello_cuda.cu         # CUDA å…¥é—¨
â”‚   â”œâ”€â”€ 02_vector_add.cu         # å‘é‡åŠ æ³•
â”‚   â”œâ”€â”€ 03_matrix_multiply.cu     # çŸ©é˜µä¹˜æ³•
â”‚   â”œâ”€â”€ 04_parallel_reduction.cu  # å¹¶è¡Œè§„çº¦
â”‚   â””â”€â”€ 05_stream_overlap.cu     # æµé‡å 
â”‚
â”œâ”€â”€ triton_basics/                # Triton ç¼–ç¨‹åŸºç¡€
â”‚   â”œâ”€â”€ 01_vector_add.py         # å‘é‡åŠ æ³•
â”‚   â””â”€â”€ 02_matrix_multiply.py     # çŸ©é˜µä¹˜æ³•
â”‚
â”œâ”€â”€ flash_attention/              # Flash Attention å®ç°
â”‚   â”œâ”€â”€ naive/                    # æœ´ç´ å®ç°
â”‚   â”œâ”€â”€ flash_v1/                 # Flash Attention v1
â”‚   â””â”€â”€ flash_v2/                 # Flash Attention v2 (Triton)
â”‚
â”œâ”€â”€ tiny-flash-attention/         # æ•™è‚²æ€§å®ç° (submodule)
â”œâ”€â”€ FlashMLA/                     # DeepSeek é«˜æ€§èƒ½å®ç° (submodule)
â”œâ”€â”€ benchmarks/                   # æ€§èƒ½æµ‹è¯•
â”œâ”€â”€ utils/                        # å·¥å…·å‡½æ•°
â””â”€â”€ requirements.txt              # Python ä¾èµ–
```

## å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒé…ç½®
```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n flash_attention python=3.8
conda activate flash_attention

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# åˆå§‹åŒ–å­æ¨¡å—
git submodule update --init --recursive
```

### è¿è¡Œç¤ºä¾‹
```bash
# CUDA åŸºç¡€ç¤ºä¾‹
cd cuda_basics
nvcc 01_hello_cuda.cu -o hello_cuda && ./hello_cuda

# Triton ç¤ºä¾‹
cd triton_basics
python 01_vector_add.py

# Flash Attention æµ‹è¯•
cd tiny-flash-attention/flash_attention_py
make

# FlashMLA æ€§èƒ½æµ‹è¯•
cd FlashMLA
python tests/test_flash_mla.py
```

## æ ¸å¿ƒå‚è€ƒé¡¹ç›®

### ğŸ“ [tiny-flash-attention](https://github.com/66RING/tiny-flash-attention)
æ•™è‚²å¯¼å‘çš„ Flash Attention å¤šè¯­è¨€å®ç°ï¼š
- **Python**ï¼šç®—æ³•åŸç†ç†è§£
- **Triton**ï¼šé«˜æ€§èƒ½ GPU ç¼–ç¨‹å…¥é—¨
- **CUDA**ï¼šåº•å±‚ä¼˜åŒ–å®è·µ
- **CuTLASS**ï¼šçŸ©é˜µè®¡ç®—ä¼˜åŒ–

### ğŸš€ [FlashMLA](https://github.com/deepseek-ai/FlashMLA)
DeepSeek å¼€æºçš„ç”Ÿäº§çº§ MLA å®ç°ï¼š
- **æè‡´æ€§èƒ½**ï¼šH800 SXM5 ä¸Š 660 TFLOPS
- **Hopper ä¼˜åŒ–**ï¼šä¸“é—¨é’ˆå¯¹æœ€æ–° GPU æ¶æ„
- **å˜é•¿åºåˆ—**ï¼šé«˜æ•ˆçš„å¯å˜é•¿åº¦åºåˆ—æœåŠ¡
- **å·¥ç¨‹å®è·µ**ï¼šç”Ÿäº§ç¯å¢ƒçš„æ€§èƒ½ä¼˜åŒ–

## å­¦ä¹ èµ„æº

### æ ¸å¿ƒè®ºæ–‡
- [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
- [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/abs/2307.08691)
- [Online normalizer calculation for softmax](https://arxiv.org/abs/1805.02867)

### æŠ€æœ¯æ–‡æ¡£
- [CUDA ç¼–ç¨‹æŒ‡å—](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)
- [Triton æ–‡æ¡£](https://triton-lang.org/)
- [Flash Attention å®˜æ–¹å®ç°](https://github.com/Dao-AILab/flash-attention)

### æ€§èƒ½å¯¹æ¯”

| å®ç°æ–¹å¼ | ç›®æ ‡åœºæ™¯ | æ€§èƒ½ç‰¹ç‚¹ | å­¦ä¹ ä»·å€¼ |
|----------|----------|----------|----------|
| **tiny-flash-attention** | æ•™è‚²å­¦ä¹  | æ¸…æ™°æ˜“æ‡‚ | â­â­â­â­â­ |
| **FlashMLA** | ç”Ÿäº§éƒ¨ç½² | æè‡´æ€§èƒ½ | â­â­â­â­ |
| **Flash Attention å®˜æ–¹** | ç ”ç©¶å¼€å‘ | åŠŸèƒ½å®Œæ•´ | â­â­â­ |

## è‡´è°¢

### æ ¸å¿ƒè´¡çŒ®è€…
- **[@66RING](https://github.com/66RING)** - [tiny-flash-attention](https://github.com/66RING/tiny-flash-attention) é¡¹ç›®ä½œè€…ï¼Œä¸ºæœ¬å­¦ä¹ é¡¹ç›®æä¾›äº†å®è´µçš„æ•™è‚²èµ„æº
- **[DeepSeek AI](https://github.com/deepseek-ai)** - [FlashMLA](https://github.com/deepseek-ai/FlashMLA) çš„å¼€å‘å›¢é˜Ÿï¼Œå±•ç¤ºäº†ç”Ÿäº§çº§é«˜æ€§èƒ½å®ç°

### å­¦æœ¯è‡´è°¢
- **Tri Dao** ç­‰äººçš„ Flash Attention åŸåˆ›æ€§å·¥ä½œ
- **OpenAI Triton** å›¢é˜Ÿæ¨åŠ¨çš„ GPU ç¼–ç¨‹é©æ–°
- **NVIDIA** åœ¨ CUDA ç”Ÿæ€ç³»ç»Ÿæ–¹é¢çš„æŒç»­æŠ•å…¥

## ä½¿ç”¨æŒ‡å—

### å­¦ä¹ å»ºè®®
1. **åˆå­¦è€…**ï¼šä» `docs/flash_attention_study/` çš„ç†è®ºæ–‡æ¡£å¼€å§‹
2. **è¿›é˜¶è€…**ï¼šåˆ†æ `tiny-flash-attention` çš„å¤šç§å®ç°
3. **ä¸“å®¶çº§**ï¼šç ”ç©¶ `FlashMLA` çš„æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯

### æ€§èƒ½æµ‹è¯•
```bash
# å¯¹æ¯”ä¸åŒå®ç°çš„æ€§èƒ½
cd benchmarks
python benchmark_all.py

# åˆ†ææ€§èƒ½ç“¶é¢ˆ
nsys profile python flash_attention_benchmark.py
```

### å¼€å‘å®è·µ
```bash
# å®ç°è‡ªå·±çš„ä¼˜åŒ–ç‰ˆæœ¬
cp tiny-flash-attention/flash_attention_py/tiny_flash_attn.py my_implementation.py
# åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œä¼˜åŒ–å’Œæ”¹è¿›
```

---

ğŸš€ **å¼€å§‹æ‚¨çš„é«˜æ€§èƒ½ GPU ç¼–ç¨‹å­¦ä¹ ä¹‹æ—…ï¼**

æœ¬é¡¹ç›®å°†å¸®åŠ©æ‚¨ä»åŸºç¡€æ¦‚å¿µåˆ°å‰æ²¿å®ç°ï¼Œç³»ç»ŸæŒæ¡ç°ä»£ GPU è®¡ç®—çš„æ ¸å¿ƒæŠ€æœ¯ã€‚