# CUDA & Triton Learning: Flash Attention ä¸é«˜æ€§èƒ½ç®—å­å®ç°

æœ¬é¡¹ç›®è‡´åŠ›äºæ·±å…¥å­¦ä¹ é«˜æ€§èƒ½ GPU ç®—å­çš„è®¾è®¡ä¸å®ç°ï¼Œä»¥ Flash Attention ä¸ºæ ¸å¿ƒæ¡ˆä¾‹ï¼Œé€šè¿‡åˆ†æå¤šç§å®ç°æ–¹å¼æ¥æŒæ¡ CUDA å¹¶è¡Œç¼–ç¨‹å’Œ Triton ç®—å­å¼€å‘æŠ€èƒ½ã€‚

## é¡¹ç›®ç‰¹è‰²

- **ç†è®ºä¸å®è·µç»“åˆ**ï¼šä»æ•°å­¦åŸç†åˆ°å·¥ç¨‹å®ç°çš„å®Œæ•´å­¦ä¹ è·¯å¾„
- **å¤šæ¡†æ¶å¯¹æ¯”**ï¼šPythonã€Tritonã€CUDAã€CuTLASS å››ç§å®ç°æ–¹å¼çš„æ·±åº¦åˆ†æ
- **å‰æ²¿æŠ€æœ¯æ¢ç´¢**ï¼šé›†æˆ DeepSeek FlashMLA ç­‰ä¸šç•Œæœ€æ–°é«˜æ€§èƒ½å®ç°
- **ç³»ç»Ÿæ€§å­¦ä¹ **ï¼šä¸“é—¨çš„ CuTLASS 3.x å­¦ä¹ æ¨¡å—
- **ç”Ÿäº§çº§å®è·µ**ï¼šä»ç®—æ³•åˆ°äº§å“çš„å®Œæ•´å·¥ç¨‹æ€ç»´

## ğŸ—‚ï¸ é¡¹ç›®ç»“æ„

```
cuda-triton-learning/
â”œâ”€â”€ ğŸ“š docs/                          # è¯¦ç»†å­¦ä¹ æ–‡æ¡£
â”‚   â”œâ”€â”€ flash_attention_study/        # Flash Attention ç®—æ³•æ·±åº¦å­¦ä¹ 
â”‚   â”‚   â”œâ”€â”€ 01_theory.md              # ç†è®ºåŸç†è¯¦è§£
â”‚   â”‚   â”œâ”€â”€ 02_v1_vs_v2.md            # ç‰ˆæœ¬å¯¹æ¯”åˆ†æ
â”‚   â”‚   â”œâ”€â”€ 03_implementation.md      # å¤šå®ç°æ·±åº¦åˆ†æ
â”‚   â”‚   â””â”€â”€ 04_optimization.md       # ä¼˜åŒ–æŒ‡å—
â”‚   â”‚
â”‚   â”œâ”€â”€ flashmla_study/               # FlashMLA ç”Ÿäº§çº§å®ç°
â”‚   â”‚   â”œâ”€â”€ 01_flashmla_overview.md   # DeepSeek FlashMLA æ·±åº¦è§£æ
â”‚   â”‚   â””â”€â”€ README.md                 # FlashMLA å­¦ä¹ æŒ‡å—
â”‚   â”‚
â”‚   â”œâ”€â”€ cutlass_study/                # CuTLASS 3.x é«˜æ€§èƒ½ç¼–ç¨‹
â”‚   â”‚   â”œâ”€â”€ 01_introduction.md        # CuTLASS å…¥é—¨æŒ‡å—
â”‚   â”‚   â”œâ”€â”€ 02_programming_basics.md  # CuTe ç¼–ç¨‹åŸºç¡€
â”‚   â”‚   â””â”€â”€ README.md                 # CuTLASS å­¦ä¹ æŒ‡å—
â”‚   â”‚
â”‚   â””â”€â”€ learning_plan.md              # 11å‘¨å­¦ä¹ è®¡åˆ’
â”‚
â”œâ”€â”€ ğŸ’» ä»£ç å®è·µ/                        # åŠ¨æ‰‹å®è·µä»£ç 
â”‚   â”œâ”€â”€ cuda_basics/                  # CUDA ç¼–ç¨‹åŸºç¡€
â”‚   â”‚   â”œâ”€â”€ 01_hello_cuda.cu         # CUDA å…¥é—¨
â”‚   â”‚   â”œâ”€â”€ 02_vector_add.cu         # å‘é‡åŠ æ³•
â”‚   â”‚   â”œâ”€â”€ 03_matrix_multiply.cu     # çŸ©é˜µä¹˜æ³•ä¸å…±äº«å†…å­˜
â”‚   â”‚   â”œâ”€â”€ 04_parallel_reduction.cu  # å¹¶è¡Œè§„çº¦ç®—æ³•
â”‚   â”‚   â””â”€â”€ 05_stream_overlap.cu     # æµé‡å ä¼˜åŒ–
â”‚   â”‚
â”‚   â”œâ”€â”€ triton_basics/                # Triton ç¼–ç¨‹åŸºç¡€
â”‚   â”‚   â”œâ”€â”€ 01_vector_add.py         # Triton å‘é‡åŠ æ³•
â”‚   â”‚   â””â”€â”€ 02_matrix_multiply.py     # Triton çŸ©é˜µä¹˜æ³•
â”‚   â”‚
â”‚   â”œâ”€â”€ cutlass_basics/               # CuTLASS ç¼–ç¨‹å®è·µ
â”‚   â”‚   â”œâ”€â”€ 01_gemm_basic.cu         # CuTLASS åŸºç¡€ GEMM
â”‚   â”‚   â”œâ”€â”€ 02_cute_tensor.cu        # CuTe å¼ é‡æ“ä½œ
â”‚   â”‚   â””â”€â”€ Makefile                 # ç¼–è¯‘é…ç½®
â”‚   â”‚
â”‚   â””â”€â”€ flash_attention/              # Flash Attention å®ç°
â”‚       â”œâ”€â”€ naive/                    # æœ´ç´ å®ç°
â”‚       â”œâ”€â”€ flash_v1/                 # Flash Attention v1
â”‚       â””â”€â”€ flash_v2/                 # Flash Attention v2 (Triton)
â”‚
â”œâ”€â”€ ğŸ”¬ å‚è€ƒé¡¹ç›®/                        # é«˜ä»·å€¼å‚è€ƒå®ç°
â”‚   â”œâ”€â”€ tiny-flash-attention/         # æ•™è‚²æ€§å¤šè¯­è¨€å®ç° (submodule)
â”‚   â”‚   â”œâ”€â”€ flash_attention_py/       # Python + Triton å®ç°
â”‚   â”‚   â”œâ”€â”€ flash_attention_cuda/     # CUDA å®ç°
â”‚   â”‚   â””â”€â”€ flash_attention_cutlass/  # CuTLASS å®ç°
â”‚   â”‚
â”‚   â””â”€â”€ FlashMLA/                     # DeepSeek ç”Ÿäº§çº§å®ç° (submodule)
â”‚       â”œâ”€â”€ csrc/                     # CUDA C++ + CuTLASS æ ¸å¿ƒ
â”‚       â”œâ”€â”€ flash_mla/                # Python æ¥å£
â”‚       â””â”€â”€ benchmark/                # æ€§èƒ½æµ‹è¯•
â”‚
â”œâ”€â”€ ğŸ§ª å®éªŒå·¥å…·/                        # è¾…åŠ©å·¥å…·
â”‚   â”œâ”€â”€ benchmarks/                   # æ€§èƒ½æµ‹è¯•
â”‚   â”œâ”€â”€ utils/                        # å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ requirements.txt              # Python ä¾èµ–
â”‚
â””â”€â”€ ğŸ“‹ é…ç½®æ–‡ä»¶/                        # é¡¹ç›®é…ç½®
    â”œâ”€â”€ setup.md                      # ç¯å¢ƒé…ç½®æŒ‡å—
    â””â”€â”€ README.md                     # æœ¬æ–‡ä»¶
```

## ğŸ¯ å­¦ä¹ è·¯å¾„

### ğŸ”° åŸºç¡€è·¯å¾„ (æ–°æ‰‹å‹å¥½)

é€‚åˆï¼šCUDA/GPU ç¼–ç¨‹åˆå­¦è€…

```
Week 1-2: CUDA åŸºç¡€
â”œâ”€â”€ cuda_basics/ ç¤ºä¾‹å­¦ä¹ 
â”œâ”€â”€ docs/learning_plan.md å‰4å‘¨å†…å®¹
â””â”€â”€ å®è·µï¼šåŸºç¡€ CUDA ç¨‹åº

Week 3-4: Flash Attention ç†è®º
â”œâ”€â”€ docs/flash_attention_study/01_theory.md
â”œâ”€â”€ flash_attention/naive/ å®ç°
â””â”€â”€ å®è·µï¼šæœ´ç´  Attention å®ç°

Week 5-6: Triton å…¥é—¨
â”œâ”€â”€ triton_basics/ ç¤ºä¾‹
â”œâ”€â”€ flash_attention/flash_v2/ Triton å®ç°
â””â”€â”€ å®è·µï¼šTriton Flash Attention
```

### ğŸ”¥ è¿›é˜¶è·¯å¾„ (æŠ€æœ¯æ·±å…¥)

é€‚åˆï¼šæœ‰ CUDA åŸºç¡€çš„å¼€å‘è€…

```
Week 1-3: Flash Attention å…¨æ™¯
â”œâ”€â”€ docs/flash_attention_study/ å®Œæ•´å­¦ä¹ 
â”œâ”€â”€ tiny-flash-attention/ å¤šå®ç°å¯¹æ¯”
â””â”€â”€ å®è·µï¼šæ€§èƒ½å¯¹æ¯”åˆ†æ

Week 4-6: CuTLASS æ·±åº¦å­¦ä¹ 
â”œâ”€â”€ docs/cutlass_study/ ç³»ç»Ÿå­¦ä¹ 
â”œâ”€â”€ cutlass_basics/ å®è·µç¼–ç¨‹
â””â”€â”€ å®è·µï¼šè‡ªå®šä¹‰ GEMM å®ç°

Week 7-8: FlashMLA ç”Ÿäº§å®è·µ
â”œâ”€â”€ docs/flashmla_study/ æ·±åº¦è§£æ
â”œâ”€â”€ FlashMLA/ æºç åˆ†æ
â””â”€â”€ å®è·µï¼šæ€§èƒ½ä¼˜åŒ–å®éªŒ
```

### ğŸš€ ä¸“å®¶è·¯å¾„ (å‰æ²¿æŠ€æœ¯)

é€‚åˆï¼šGPU é«˜æ€§èƒ½è®¡ç®—ä¸“å®¶

```
å¹¶è¡Œå­¦ä¹ :
â”œâ”€â”€ FlashMLA æºç æ·±åº¦åˆ†æ
â”œâ”€â”€ CuTLASS 3.x é«˜çº§ç‰¹æ€§
â”œâ”€â”€ Hopper æ¶æ„ä¼˜åŒ–æŠ€æœ¯
â””â”€â”€ è‡ªå®šä¹‰ç®—å­å¼€å‘å®è·µ

é¡¹ç›®ç›®æ ‡:
â”œâ”€â”€ å®ç°ç”Ÿäº§çº§ Attention å˜ä½“
â”œâ”€â”€ ä¼˜åŒ–ç°æœ‰ç®—å­å®ç°
â”œâ”€â”€ è´¡çŒ®å¼€æºé¡¹ç›®
â””â”€â”€ å‘è¡¨æŠ€æœ¯åšå®¢
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒé…ç½®

```bash
# 1. å…‹éš†é¡¹ç›®
git clone <your-repo-url>
cd cuda-triton-learning

# 2. åˆå§‹åŒ–å­æ¨¡å—
git submodule update --init --recursive

# 3. é…ç½® Python ç¯å¢ƒ
conda create -n flash_attention python=3.8
conda activate flash_attention
pip install -r requirements.txt

# 4. éªŒè¯ CUDA ç¯å¢ƒ
nvcc --version  # éœ€è¦ CUDA 11.0+
nvidia-smi      # æ£€æŸ¥ GPU
```

### éªŒè¯å®‰è£…

```bash
# æµ‹è¯• CUDA åŸºç¡€
cd cuda_basics
nvcc 01_hello_cuda.cu -o hello_cuda && ./hello_cuda

# æµ‹è¯• Triton
cd ../triton_basics
python 01_vector_add.py

# æµ‹è¯• tiny-flash-attention
cd ../tiny-flash-attention/flash_attention_py
make

# æµ‹è¯• FlashMLA (éœ€è¦ Hopper GPU)
cd ../../FlashMLA
python tests/test_flash_mla.py
```

## ğŸ”¬ æ ¸å¿ƒå‚è€ƒé¡¹ç›®

### ğŸ“ [tiny-flash-attention](https://github.com/66RING/tiny-flash-attention)
**æ•™è‚²å¯¼å‘çš„å¤šè¯­è¨€å®ç°**

**ä»·å€¼**ï¼š
- **Python**ï¼šç®—æ³•åŸç†ç†è§£çš„æœ€ä½³èµ·ç‚¹
- **Triton**ï¼šé«˜æ€§èƒ½ GPU ç¼–ç¨‹å…¥é—¨
- **CUDA**ï¼šåº•å±‚ä¼˜åŒ–å®è·µ
- **CuTLASS**ï¼šæ¨¡æ¿åŒ–é«˜æ€§èƒ½è®¡ç®—

**å­¦ä¹ é‡ç‚¹**ï¼š
```python
# ä»ç®€å•åˆ°å¤æ‚çš„å­¦ä¹ è·¯å¾„
tiny-flash-attention/
â”œâ”€â”€ flash_attention_py/     # ä»è¿™é‡Œå¼€å§‹
â”œâ”€â”€ flash_attention_triton/ # ç„¶åå­¦ä¹  Triton
â”œâ”€â”€ flash_attention_cuda/   # æ·±å…¥ CUDA ä¼˜åŒ–
â””â”€â”€ flash_attention_cutlass/ # æœ€åæŒæ¡ CuTLASS
```

### ğŸš€ [FlashMLA](https://github.com/deepseek-ai/FlashMLA)
**DeepSeek çš„ç”Ÿäº§çº§ MLA å®ç°**

**ä»·å€¼**ï¼š
- **æè‡´æ€§èƒ½**ï¼šH800 SXM5 ä¸Š 660 TFLOPS
- **Hopper ä¼˜åŒ–**ï¼šæœ€æ–° GPU æ¶æ„æ·±åº¦åˆ©ç”¨
- **å·¥ç¨‹å®è·µ**ï¼šç”Ÿäº§ç¯å¢ƒçš„å®Œæ•´è€ƒé‡
- **æŠ€æœ¯å‰æ²¿**ï¼šMLA æ¶æ„çš„æœ€ä½³å®ç°

**å­¦ä¹ é‡ç‚¹**ï¼š
```cpp
// ç”Ÿäº§çº§ä»£ç çš„å·¥ç¨‹å®è·µ
FlashMLA/
â”œâ”€â”€ csrc/kernels/        # æ ¸å¿ƒ Kernel å®ç°
â”œâ”€â”€ csrc/cutlass/        # CuTLASS 3.x åº”ç”¨
â”œâ”€â”€ flash_mla/           # Python æ¥å£è®¾è®¡
â””â”€â”€ benchmark/           # æ€§èƒ½æµ‹è¯•ä½“ç³»
```

## ğŸ“Š æŠ€æœ¯å¯¹æ¯”åˆ†æ

### å®ç°æ–¹å¼å¯¹æ¯”

| å®ç°æ–¹å¼ | å­¦ä¹ éš¾åº¦ | æ€§èƒ½æ°´å¹³ | å·¥ç¨‹ä»·å€¼ | æ¨èæŒ‡æ•° |
|----------|----------|----------|----------|----------|
| **Python (Naive)** | â­ | â­ | â­â­ | â­â­â­â­â­ |
| **Triton** | â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| **CUDA** | â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ |
| **CuTLASS** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­ |
| **FlashMLA** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |

### å­¦ä¹ ä»·å€¼åˆ†æ

| æŠ€æœ¯æ–¹å‘ | æ ¸å¿ƒä»·å€¼ | åº”ç”¨åœºæ™¯ | èŒä¸šå‘å±• |
|----------|----------|----------|----------|
| **Flash Attention ç®—æ³•** | ç†è®ºåŸºç¡€ | ç ”ç©¶å¼€å‘ | ç®—æ³•å·¥ç¨‹å¸ˆ |
| **Triton ç¼–ç¨‹** | é«˜æ•ˆå¼€å‘ | å¿«é€ŸåŸå‹ | AI ç³»ç»Ÿå·¥ç¨‹å¸ˆ |
| **CUDA ä¼˜åŒ–** | æ·±åº¦æ§åˆ¶ | æè‡´æ€§èƒ½ | GPU ç¼–ç¨‹ä¸“å®¶ |
| **CuTLASS å·¥ç¨‹** | ç”Ÿäº§çº§è´¨é‡ | å¤§è§„æ¨¡éƒ¨ç½² | é«˜æ€§èƒ½è®¡ç®—ä¸“å®¶ |
| **FlashMLA å®è·µ** | å‰æ²¿æŠ€æœ¯ | å•†ä¸šåº”ç”¨ | æŠ€æœ¯é¢†å¯¼è€… |

## ğŸ› ï¸ å®è·µé¡¹ç›®æ¨è

### Level 1: åŸºç¡€å®éªŒ

```python
# é¡¹ç›® 1: Flash Attention æ€§èƒ½å¯¹æ¯”
ç›®æ ‡: ç†è§£ä¸åŒå®ç°çš„æ€§èƒ½ç‰¹å¾
ä»»åŠ¡: 
- å®ç°æœ´ç´  Attention
- ä½¿ç”¨ Triton ä¼˜åŒ–
- å¯¹æ¯”åˆ†ææ€§èƒ½å·®å¼‚
- ç»˜åˆ¶æ€§èƒ½æ›²çº¿å›¾
```

### Level 2: è¿›é˜¶å¼€å‘

```cpp
// é¡¹ç›® 2: è‡ªå®šä¹‰ CUDA Kernel
ç›®æ ‡: æŒæ¡ CUDA ä¼˜åŒ–æŠ€æœ¯
ä»»åŠ¡:
- å®ç° Flash Attention CUDA ç‰ˆæœ¬
- åº”ç”¨å…±äº«å†…å­˜ä¼˜åŒ–
- ä½¿ç”¨ Tensor Core åŠ é€Ÿ
- æ€§èƒ½è°ƒä¼˜å’Œåˆ†æ
```

### Level 3: é«˜çº§å·¥ç¨‹

```cpp
// é¡¹ç›® 3: CuTLASS ç®—å­å¼€å‘
ç›®æ ‡: å­¦ä¹ ç”Ÿäº§çº§å¼€å‘
ä»»åŠ¡:
- åŸºäº CuTLASS å®ç°è‡ªå®šä¹‰ç®—å­
- åº”ç”¨ CuTe å¼ é‡æŠ½è±¡
- é›†æˆ TMA å†…å­˜åŠ é€Ÿ
- å¯¹æ ‡ FlashMLA æ€§èƒ½
```

### Level 4: å‰æ²¿æ¢ç´¢

```cpp
// é¡¹ç›® 4: FlashMLA æ‰©å±•
ç›®æ ‡: æ¨åŠ¨æŠ€æœ¯è¾¹ç•Œ
ä»»åŠ¡:
- åˆ†æ FlashMLA æºç 
- å®ç°æ–°çš„ Attention å˜ä½“
- ä¼˜åŒ–ç‰¹å®šåº”ç”¨åœºæ™¯
- è´¡çŒ®å¼€æºç¤¾åŒº
```

## ğŸ“š å­¦ä¹ èµ„æº

### ğŸ“– æ ¸å¿ƒæ–‡æ¡£

æœ¬é¡¹ç›®æä¾›ä¸‰ä¸ªä¸“é—¨çš„å­¦ä¹ æŒ‡å—ï¼š

1. **[Flash Attention å­¦ä¹ æŒ‡å—](./docs/flash_attention_study/)** - ç®—æ³•ç†è®ºå’ŒåŸºç¡€å®ç°
2. **[FlashMLA å­¦ä¹ æŒ‡å—](./docs/flashmla_study/)** - ç”Ÿäº§çº§å®ç°å’Œå·¥ç¨‹å®è·µ  
3. **[CuTLASS å­¦ä¹ æŒ‡å—](./docs/cutlass_study/)** - é«˜æ€§èƒ½ç¼–ç¨‹æ¡†æ¶

### ğŸ“„ æŠ€æœ¯è®ºæ–‡

- [FlashAttention: Fast and Memory-Efficient Exact Attention](https://arxiv.org/abs/2205.14135)
- [FlashAttention-2: Faster Attention with Better Parallelism](https://arxiv.org/abs/2307.08691)
- [Multi-Head Latent Attention](https://arxiv.org/abs/2410.04343)
- [Online normalizer calculation for softmax](https://arxiv.org/abs/1805.02867)

### ğŸ”— æŠ€æœ¯æ–‡æ¡£

- [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)
- [Triton å®˜æ–¹æ–‡æ¡£](https://triton-lang.org/)
- [CuTLASS æ–‡æ¡£](https://nvidia.github.io/cutlass/)
- [NVIDIA Hopper æ¶æ„ç™½çš®ä¹¦](https://www.nvidia.com/en-us/data-center/h100/)

## ğŸ¯ å­¦ä¹ æˆæœéªŒè¯

### ğŸ¥‰ å…¥é—¨çº§ (1-2 æœˆ)
- [ ] ç†è§£ Flash Attention çš„æ ¸å¿ƒåŸç†
- [ ] èƒ½å¤Ÿè¿è¡Œå’Œä¿®æ”¹åŸºç¡€ç¤ºä¾‹
- [ ] æŒæ¡ CUDA å’Œ Triton çš„åŸºæœ¬æ¦‚å¿µ
- [ ] å®Œæˆæ€§èƒ½å¯¹æ¯”å®éªŒ

### ğŸ¥ˆ è¿›é˜¶çº§ (3-6 æœˆ)
- [ ] ç‹¬ç«‹å®ç° Flash Attention CUDA ç‰ˆæœ¬
- [ ] æŒæ¡ CuTLASS ç¼–ç¨‹åŸºç¡€
- [ ] ç†è§£ FlashMLA çš„å®ç°ç­–ç•¥
- [ ] èƒ½å¤Ÿè¿›è¡Œæ€§èƒ½è°ƒä¼˜

### ğŸ¥‡ ä¸“å®¶çº§ (6+ æœˆ)
- [ ] åŸºäº CuTLASS å¼€å‘è‡ªå®šä¹‰ç®—å­
- [ ] æ·±åº¦ç†è§£ FlashMLA çš„ä¼˜åŒ–æŠ€æœ¯
- [ ] èƒ½å¤Ÿè§£å†³å¤æ‚çš„æ€§èƒ½é—®é¢˜
- [ ] å…·å¤‡æŒ‡å¯¼ä»–äººçš„èƒ½åŠ›

## ğŸ¤ è´¡çŒ®æŒ‡å—

### æ¬¢è¿è´¡çŒ®

1. **ä»£ç ç¤ºä¾‹**ï¼šæ›´å¤šå®è·µæ€§çš„ä»£ç ç¤ºä¾‹å’Œæ•™ç¨‹
2. **æ€§èƒ½åˆ†æ**ï¼šä¸åŒç¡¬ä»¶å¹³å°çš„æ€§èƒ½æµ‹è¯•ç»“æœ
3. **æ–‡æ¡£å®Œå–„**ï¼šæ”¹è¿›ç°æœ‰æ–‡æ¡£ï¼Œæ·»åŠ æ–°çš„å­¦ä¹ ææ–™
4. **é—®é¢˜è§£ç­”**ï¼šå¸®åŠ©å…¶ä»–å­¦ä¹ è€…è§£å†³æŠ€æœ¯é—®é¢˜

### æäº¤æ–¹å¼

```bash
# Fork é¡¹ç›®å¹¶åˆ›å»ºæ–°åˆ†æ”¯
git checkout -b feature/your-contribution

# æäº¤æ›´æ”¹
git commit -m "Add: your contribution description"

# åˆ›å»º Pull Request
# è¯·è¯¦ç»†æè¿°æ‚¨çš„è´¡çŒ®å†…å®¹å’Œä»·å€¼
```

## ğŸ† è‡´è°¢

### ğŸ“ æ•™è‚²èµ„æºè´¡çŒ®è€…
- **[@66RING](https://github.com/66RING)** - [tiny-flash-attention](https://github.com/66RING/tiny-flash-attention) é¡¹ç›®ï¼Œæä¾›äº†æ— ä»·çš„æ•™è‚²èµ„æº

### ğŸš€ æŠ€æœ¯åˆ›æ–°è´¡çŒ®è€…  
- **[DeepSeek AI](https://github.com/deepseek-ai)** - [FlashMLA](https://github.com/deepseek-ai/FlashMLA) é¡¹ç›®ï¼Œå±•ç¤ºäº†ç”Ÿäº§çº§å®ç°çš„æœ€é«˜æ ‡å‡†

### ğŸ”¬ å­¦æœ¯è´¡çŒ®è€…
- **Tri Dao** ç­‰äºº - Flash Attention çš„åŸåˆ›æ€§ç®—æ³•è´¡çŒ®
- **OpenAI Triton** å›¢é˜Ÿ - æ¨åŠ¨ GPU ç¼–ç¨‹èŒƒå¼çš„é©æ–°
- **NVIDIA CuTLASS** å›¢é˜Ÿ - é«˜æ€§èƒ½è®¡ç®—æ¨¡æ¿åº“çš„å¼€å‘

### ğŸŒŸ ç¤¾åŒºè´¡çŒ®è€…
- æ‰€æœ‰ä¸ºæœ¬é¡¹ç›®æä¾›åé¦ˆã€å»ºè®®å’Œè´¡çŒ®çš„å¼€å‘è€…å’Œç ”ç©¶è€…

---

**ğŸš€ å¼€å§‹æ‚¨çš„é«˜æ€§èƒ½ GPU ç¼–ç¨‹å­¦ä¹ ä¹‹æ—…ï¼**

ä»åŸºç¡€æ¦‚å¿µåˆ°å‰æ²¿å®ç°ï¼Œä»ç®—æ³•ç†è®ºåˆ°å·¥ç¨‹å®è·µï¼Œæœ¬é¡¹ç›®å°†å¸®åŠ©æ‚¨ç³»ç»ŸæŒæ¡ç°ä»£ GPU è®¡ç®—çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œæˆä¸ºé«˜æ€§èƒ½è®¡ç®—é¢†åŸŸçš„ä¸“å®¶ã€‚

**é€‰æ‹©æ‚¨çš„å­¦ä¹ è·¯å¾„ï¼Œæ¢ç´¢ GPU è®¡ç®—çš„æ— é™å¯èƒ½ï¼**

*æœ€åæ›´æ–°ï¼š2024å¹´12æœˆ - å®Œæ•´é¡¹ç›®ç»“æ„ï¼ŒåŒ…å« FlashMLA å’Œ CuTLASS ä¸“é—¨æ¨¡å—*